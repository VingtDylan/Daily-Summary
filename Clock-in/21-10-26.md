**日期**: 2021年10月26日 星期二      **姓名**: 陈勇虎 

**Plan:**

- [ ] 学习四个Normalization方法

**Do**:

- [ ] 学习四个Normalization方法

**Check**:

​		白化的计算成本太高，于是有了normalization方法，可以简化计算过程，又可以使数据尽可能保留原始的数据表达能力。normalization这种方式，实际上并不是直接去解决ICS问题，更多的是面向梯度消失等问题，去加速网络收敛的。而且，均值方差相等与同分布不是一个概念。

**Normalization Framework**
$$
\hat{x} = f(g*\frac{x-\mu}{\sigma} + b) \notag
$$

​		通常的变换框架如上所示，实际上就是做了个shift和缩放转换到(0,1)正态分布，随后再进行reshift和rescale，最终转换成了 ($b$, $g^2$) 的分布。这里的reshift和rescale参数是可以学习的，而不是固定的(0,1)，很显然，因为输入范围本身就不可能确定为(0,1)，可学习的re参数也更符合底层的学习能力。

与白化相比，实际差距还是很大的。

**Batch Normalization**

![](https://pic1.zhimg.com/80/v2-13bb64b6122e98421ea3528539c1bffc_720w.jpg)

​		其规范化针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 的均值和方差,因而称为 Batch Normalization。

<img src="D:\MyBlog\VingtDylan\source\_posts\Internal-Covariate-Shift-Normalization\ICSN1.png" style="zoom:50%;" />

​		BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。

BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于动态的网络结构 和 RNN 网络。

**Layer Normalization**

![](https://pic1.zhimg.com/80/v2-2f1ad5749e4432d11e777cf24b655da8_720w.jpg)

​		LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。

**Instance Normalization**

<img src="D:\MyBlog\VingtDylan\source\_posts\Internal-Covariate-Shift-Normalization\ICSN2.png" style="zoom:50%;" />

​		IN适用于生成模型中，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，更适合对单个像素有更高要求的场景。

**Group Normalization**

<img src="D:\MyBlog\VingtDylan\source\_posts\Internal-Covariate-Shift-Normalization\ICSN3.png" style="zoom:50%;" />

​		Kaiming He的论文中以图片的形式说明了四个的区别，Group介于IN和LN之间。

**Action**:

- [ ] 继续调研光流法动态感知领域的应用算法和光流估计算法
- [ ] 阅读和学习论文源码
- [ ] 调研和收集Transformer的应用
- [ ] 调研和收集处理occlusion问题的解决方案

**Reference**

- [ ] Ioffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” *ArXiv:1502.03167 [Cs]*, March. http://arxiv.org/abs/1502.03167.
- [ ] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” *ArXiv:1607.06450 [Cs, Stat]*, July. http://arxiv.org/abs/1607.06450.
- [ ] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. 2017. “Instance Normalization: The Missing Ingredient for Fast Stylization.” *ArXiv:1607.08022 [Cs]*, November. http://arxiv.org/abs/1607.08022.
- [ ] Wu, Yuxin, and Kaiming He. 2018. “Group Normalization.” *ArXiv:1803.08494 [Cs]*, June. http://arxiv.org/abs/1803.08494.
