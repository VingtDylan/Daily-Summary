**日期**: 2021年11月18日 星期四      **姓名**: 陈勇虎 

**Plan:**

- [ ] Automatic NetWork Search

**Do**:

- [ ] Automatic NetWork Search

**Check**:

- [ ] Automatic NetWork Search

  网络搜索时探索和优化网络架构中的一个非常强大的工具[...]，对于MobileNet v3，使用了平台感知的NAS，通过优化每个网络来搜索全局网络结构，然后使用了NetAdapt算法来搜索每一层的过滤器数量。这些技术是互补的，并且可以结合起来有效地找到针对于给定硬件平台的优化模型。
  
  **Platform-Aware NAS for Block-wise Search**
  
  这里直接使用了 MnasNet-A1来作为初始的大型移动模型，然后在其上应用 NetAdapt和其他的优化算法。
  
  但是最初的奖励函数的设计并没有针对小型移动模型进行优化，具体的来说，它使用的是多目标的奖励函数：$ACC(m)\times[LAT(m)/TAR]^w$ 来近似帕累托最优解(Pareto-optimal solutions)，并基于每个目标延迟 $TAR$ 对每个模型 $m$ 平衡模型精度 $ACC(m)$ 和延迟 $LAT(m)$。对于小模型，精度随延迟的变化显著，因此需要一个更小的权重因子 $w = -0.15$ (origin = -0.07)，以补偿不同延迟下较大的精度变化。在新的权重因子 $w$的增强加，从头构建一个新的架构，以找到一个初始种子模型，然后应用 NetAdapt和其他优化手段来获得最终的 MobileNetV3-Small模型。
  
  **NetAdapt for Layer-wise Search**
  
  架构搜索中使用的第二个技术是 NetAdapt，这是对平台感知的NAS的补充，它允许以顺序的方式微调各个层，而不是试图推断粗糙但全局的架构。具体可看原文。简而言之，这个过程是这样运行的：
  
  * 首先从平台感知的NAS找到的种子网络架构开始
  * 然后对于每一步：
    * 生成一组proposals，每个proposal 代表了对体系结构的修改，与前一步相比，延迟至少减少了 $\delta$
    * 对于每个proposal，使用前一步中得到的预训练的模型，并填充新提出的架构，适当的裁剪和随机初始化缺失的权值。对每一项proposal进行微调 $T$ 次，以得到精度的粗略估计。
    * 根据指标选出最佳的proposal。
  * 迭代每一步直到达到目标延迟。
  
  原文中的评价指标是最小化精度的变化。这里对该算法进行改进，使延迟变化和精度变化比例最小化。也就是说，对于 NetAdapt中的每一步中生成的所有的proposals，选择其中最大化 $\frac{\Delta Acc}{\Delta latency}$ 的一项，并且满足 $\Delta Acc$ 满足前面提到的 $\delta$ 减少的限制。直觉上，proposal 是离散的，因此更倾向于能够使 trade-off 曲线斜率最大的 proposal。
  
  这个过程重复进行，直到达到目标的延迟，然后从头开始训练新的体系结构。对 MobileNet v2使用与原始 NetAdapt相同proposal生成器。具体来说，允许两类proposals。
  
  * 减少 expansion layer的尺寸
  * 减少所有共享相同 bottleneck 大小的块中的 bottleneck----以保持残差连接。
  
  实验中设置 $T = 10000$，虽然增加了初始微调的准确性，但是从零开始训练时，它不会改变最终结果的准确性。设置 $\delta = 0.01|L|$，这里 $L$ 是种子模型的延迟。

**Action**:

- [ ] 调研和学习轻量化模型

