%!TEX program = xelatex
\documentclass[cn,hazy,black,normal]{elegantnote}
\usepackage{array}
\usepackage{zhnumber}
\usepackage{footmisc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{amsopn}

%公式按照章节编号
\numberwithin{equation}{section}
%图片按照章节编号
\numberwithin{figure}{section}

\title{zzh 机器学习}
\author{cyh \\ 
	SIMIT}

%\date{\zhtoday}
\date{\zhdate{2021/9/22}}

\begin{document}
	\maketitle
	
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{绪论}
	
	\section{模型评估与选择}
		\subsection{经验误差与过拟合}
			\textbf{错误率}: 分类错误的样本数占样本总数的比例，如果在 $m$ 个样本中有 $a$ 个样本分类错误，则错误率 $E = a / m$，相应的 $1 - a / m$ 称为精度(\textbf{accuracy})。
	
			学习器的实际预测输出与样本的真实输出之间的差异称为“\textbf{误差}\footnote{这里的误差指的是误差期望。}”(\textbf{error})，学习器在训练集上的误差称为“\textbf{训练误差}”(\textbf{training error})或“\textbf{经验误差}”(\textbf{empirical error})，在新样本上的误差称为 “\textbf{泛化误差}” (\textbf{generalization error})。
	
			\textbf{过拟合和欠拟合}：当学习器把训练样本学的“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本会具有的一般性质，这样就会导致泛化性能下降。这种现象在机器学习中称为“\textbf{过拟合}”(\textbf{overfitting})。与“过拟合”相对的是“\textbf{欠拟合}”(\textbf{underfitting})，这是指对训练样本的一般性质尚未学好。
	
			有很多因素可能导致过拟合，其中最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则通常是由于学习能力低下而造成的。欠拟合比较容易克服，例如在决策树学习中扩展分支，在神经网络学习中增加训练轮数等。而过拟合比较麻烦，过拟合是机器学习面临的关键障碍，各类学习算法都必然带有一些针对过拟合的措施；然而必须认识到，过拟合是无法彻底避免的，所做的只能是“缓解”，或者说是减少其风险。 

		\subsection{评估方法}
			通常，可以通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个“测试集”(testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”(testing error)作为泛化误差的近似。需要注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现，未在训练过程中使用过。
			
			对于一个包含$m$个样例的数据集$D = \{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$,需要通过对$D$进行适当的处理，从中产生出训练集S和测试集T。常用的做法如下。
			
			\subsubsection{留出法}
				“\textbf{留出法}”(hold out)直接将数据集$D$划分为两个互斥的集合，其中一个集合作为训练集$S$，另一个集合作为测试集$T$，即$D=S\cup T, S\cap T=\varnothing$。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。
				
				需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程中引入额外的偏差而对最终结果产生影响，例如在分类任务中至少保持样本的类别比例相似。如果从采样(sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”(stratified sampling)。若$S$，$T$中样本类别比例差距很大，则误差估计将由于训练/测试数据分布的差异而产生误差。
			
				另一个需注意的问题是，即便在给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集$D$进行分割。不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差异。因此，单次使用留出法得到的估计结果往往是不够稳定可靠，在使用留出法时，一般要采用若干次随即划分，重复进行实验评估后取平均值作为留出法的评估结果。
				
				在划分问题上没有完美的解决方案，常见的做法是将大约$2/3 \sim 4/5$的样本用于训练，剩余样本用于测试。
				
			\subsubsection{交叉验证法}
				“\textbf{交叉验证法}”(cross validation)先将数据集$D$划分为$k$个大小相似的互斥子集，即$D=D_1\cup D_2\cup ...\cup D_k, D_i\cap D_j =\varnothing(i\neq j)$。每个子集$D_i$都尽可能保持数据分布的一致性，即从$D$中通过分层采样得到。然后，每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集；这样就可以获得$k$组训练/测试集，从而可进行$k$次训练和测试，最终返回的是这k个测试结果的均值。交叉验证法的评估结果的稳定性和保真性在很大程度上取决于$k$的取值，通常把交叉验证法称为“\textbf{$k$折交叉验证}”(k-fold cross validation)。$k$最常用的取值是10，此时称为10折交叉验证；其他常用的$k$值有5、20等。
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=.7\linewidth]{images/10-fold.png}
					\caption{{\rm 10折交叉验证示意图}}
					\label{10-fold}
				\end{figure}
			
				与留出法相似，将数据集$D$划分为$k$个子集同样存在多种划分方式。为减少因样本划分不同而引入的差别，$k$折交叉验证通常要随机使用不同的划分重复$p$次，最终的评估结果是这$p$次$k$折交叉验证结果的均值，常用的有“10次10折交叉验证”。
	
				假定数据集$D$中包含$m$个样本，若令$k=m$，则得到了交叉验证法的一个特例：留一法(Leave-One-Out,LOO)。留一法不受随机样本划分方式的影响。但是数据集比较大的时候，计算开销难以估量。
				
			\subsubsection{自助法}
				“\textbf{自助法}”(bootstrapping)以自助采样法(bootstrapping sampling)为基础。给定包含$m$个样本的数据集$D$，对它进行采样产生数据集$D^{\prime}$:每次随机从$D$中挑选一个样本，将其拷贝放入$D^{\prime}$，然后再将该样本放回初始数据集$D$中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行$m$次后，就可以得到了一个包含$m$个样本的数据集$D^{\prime}$，这就是自主采样的结果。
				
				
				$D$中的一部分样本会在$D^{\prime}$中多次出现，而另一部分样本不出现。样本在$m$次采样中始终不被采到的概率为$(1-\frac{1}{m})^m$，取极限为:
				
				\begin{equation}
					\lim\limits_{m\mapsto\infty}(1-\frac{1}{m})^m \mapsto \frac{1}{e} \approx 0.368
				\end{equation}
	
				即通过自主采样，初始数据集$D$中约有$36.8\%$的有样本未在采样数据集$D^{\prime}$中。于是我们可将$D^{\prime}$用作训练集，$D\backslash D^{\prime}$用作测试集；这样，实际评估的模型与期望评估的模型都使用$m$个训练样本，而我们仍有数据总量约有1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称“\textbf{包外估计}”(out-of-bag estimate)。
				
				自助法在数据集较小，难以有效划分训练集/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等有很大的好处。然而，自助法产生的数据集改变了原始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。
	
			\subsubsection{调参与最终模型}
				现实中常用的做法，是对每个参数选定一个范围和变化步长。
				
				测试集上的判别效果用来估计模型在实际使用中时的泛化能力，而把训练数据另外划分为训练集和验证集，基于验证集上的性能来进行模型选择和调参。		
		
		\subsection{性能度量}
			对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure)。性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。
			
			回归任务最常用的性能度量时“均方误差”(mean squared error)。
			
			\begin{equation}
				E(f;D)=\frac{1}{m}\sum_{i = 1}^{m}(f(\boldsymbol{x}_i)-y_i)^2.
			\end{equation}
			
			更一般的，对于数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$,均方误差可描述为
			
			\begin{equation}
				E(f;\mathcal{D})=\int_{\boldsymbol{x}\sim \mathcal{D}}(f(\boldsymbol{x})-y)^2p(\boldsymbol{x})d\boldsymbol{x}.
			\end{equation}
	
			\subsubsection{错误率与精度}
				错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例。对样例集$D$。分类错误率定义为
				
				\begin{equation}
					E(f;D)=\frac{1}{m}\sum_{i = 1}^{m}\mathbb{I}(f(\boldsymbol{x}_i)\neq y_i) .
				\end{equation}
			
				精度则定义为
				
				\begin{align}
					acc(f;D) & = \frac{1}{m}\sum_{i = 1}^{m}\mathbb{I}(f(\boldsymbol{x}_i)= y_i) \\
							 & = 1 - E(f;D)\notag .
				\end{align}
	
				更一般的，对于数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，错误率与精度可分别描述为
				
				\begin{equation}
					E(f;\mathcal{D}) = \int_{\boldsymbol{x}\sim \mathcal{D}}\mathbb{I}(f(\boldsymbol{x})\neq y)p(\boldsymbol{x})d\boldsymbol{x} ,
				\end{equation}
	
				\begin{align}
					acc(f,\mathcal{D}) & = \int_{\boldsymbol{x}\sim \mathcal{D}}\mathbb{I}(f(\boldsymbol{x})= y)p(\boldsymbol{x})d\boldsymbol{x} \\
				          	 & = 1 - E(f;\mathcal{D})\notag .
				\end{align}
			
			\subsubsection{查准率，查全率与$F1$}
				查准率(precision)亦称准确率,查全率(recall)亦称召回率。
				
				TODO
	
	\section{线性模型}
			
	\section{}
	
	\section{}
	
	\section{}
	
	\section{}
		
	\section{集成学习}		 
		\subsection{个体与集成}
			集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning)等。
			
			图\ref{ensemble-learning}显示出了集成学习的一般结构：先产生一组“个体学习器”(individual learner)，再用某种策略将它们结合起来。个体学习器通常用一个现有的学习算法从训练数据产生，此时集成中只包含同种类型的个体学习器，这样的集成是“同质”的(homogeneous)，同质集成中的个体学习器亦称“基学习器”(base learner)，相应的学习算法称为“基学习算法”(base learning algorithm).集成可以包含不同类型的个体学习器，这样的集成是“异质”的(heterogenous)。异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法；相应的，个体学习器一般不称为基学习器，常称为“组件学习器”(component learner)或直接称为个体学习器。
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=.6\linewidth]{images/ensemble-learning.png}
				\caption{{\rm 集成学习示意图}}
				\label{ensemble-learning}
			\end{figure}
			
			集成学习的结果采用投票法(voting)产生，即“少数服从多数”。要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”(diversity)，即学习器间具有差异。
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=.8\linewidth]{images/ensemble-learning2.png}
				\caption{{\rm 继承个体应“好而不同”($h_i$代表第$i$个分类器)}}
				\label{ensemble-learning2}
			\end{figure}
			
			根据个体学习器的生成方式，目前的集成学习方法大概可分为两大类，即个体学习器间存在强依赖关系、必须串行产生的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是Boosting，后者的代表是Bagging和“随机森林”(Random Forest)。

		\subsection{Boosting}
			Boosting是一族可将弱学习器提升为强学习器的算法。工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权求和。
			
			\begin{figure}[H]
				\centering
				\includegraphics[width=.7\linewidth]{images/Adaboost.png}
				\caption{{\rm AdaBoost算法}}
				\label{AdaBoost}
			\end{figure}
			
			\href{https://zhuanlan.zhihu.com/p/41536315}{知乎解读}
			
		\subsection{Bagging与随机森林}	
		
		\subsection{结合策略}
		
		\subsection{多样性}
			
		
			
	\section{}	
			
			
	
\end{document}
